{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7e8b90-1513-4294-9584-d72bb9d391b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip3.11 install unsloth\\npip3.11 install transformers==4.56.2\\npip3.11 install --no-deps trl==0.22.2\\npip3.11 install sentencepiece protobuf datasets==4.3.0 \"huggingface_hub>=0.34.0\" hf_transfer\\npip3.11 install accelerate\\npip3.11 install peft\\npip3.11 install bitsandbytes\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Install these packages\n",
    "\"\"\"\n",
    "pip3.11 install unsloth\n",
    "pip3.11 install transformers==4.56.2\n",
    "pip3.11 install --no-deps trl==0.22.2\n",
    "pip3.11 install sentencepiece protobuf datasets==4.3.0 \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "pip3.11 install accelerate\n",
    "pip3.11 install peft\n",
    "pip3.11 install bitsandbytes\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e048bcb8-46da-4fe0-b135-5433780f820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-14B\",\n",
    "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = True,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, \n",
    "    # token = \"hf_...\",      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2cd19-5283-48fb-a1ea-6bb464a9a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA Adapters to only update few parameters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,           \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,  \n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  \n",
    "    loftq_config = None,  # And LoftQ\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.11 (vlms)",
   "language": "python",
   "name": "vlms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
